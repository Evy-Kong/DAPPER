{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from resources.workspace import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial introduces the Kalman filter (KF)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A straight-line example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many mathematical methods are tagged with the label \"least squares\". They typically have one thing in common: some sum of squared terms is being minimized. Both linear regression and Kalman filtering (KF) are a form of \"least squares\".\n",
    "Do they yield the same estimate? That's what we'll investigate...\n",
    "\n",
    "Consider the straight line\n",
    "$$x_k = a k \\, ,    \\qquad \\qquad (1) $$\n",
    "where $k$ is an index, usually of time.\n",
    "Now suppose we have noisy observations of this line:\n",
    "\\begin{align*}\n",
    "y_k &= x_k + \\xi_k \\, . \\qquad \\qquad (2)\n",
    "\\end{align*}\n",
    "\n",
    "(Least-squares) linear regression minimizes\n",
    "$$J_K(\\hat{a}) = \\sum_{k=1}^K (y_k - \\hat{a} k)^2 \\, ,  \\qquad \\qquad (4)$$\n",
    "yielding the estimator\n",
    "$$\\hat{a}_K = \\frac{\\sum_{k=1}^K {k} y_{k}}{\\sum_{k=1}^K {k}^2} \\, . \\qquad \\qquad (6)$$\n",
    "\n",
    "**Exc 3.2:** Derive (6) from (4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('LinReg deriv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below sets up an experiment based on eqns. (1) and (2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = 0.4  # Slope (xx[k] = a*k) paramterer. Unknown to be estimated.\n",
    "K = 10   # Length of experiment (final time index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q = 0    # Dynam model noise strength\n",
    "R = 1    # Observation noise strength\n",
    "H = 1    # Observation operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate synthetic truth (`xx`) and observations (`yy`). Again, the naming convention indicates that these variables hold time series of `x` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Note: Python indexing starts at 0.\n",
    "# Convention: no obs at k==0.\n",
    "xx = np.zeros(K+1) # states\n",
    "yy = np.zeros(K)   # obs\n",
    "\n",
    "for k in range(K):\n",
    "    xx[k+1] = a*(k+1)\n",
    "    yy[k]   = xx[k+1] + randn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.4:** Program the linear regresson estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lin_reg(k):\n",
    "    ### INSERT ANSWER HERE ###\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('LinReg_k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the outcome of the estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@interact(k=IntSlider(min=1, max=K))\n",
    "def plot_experiment(k):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    \n",
    "    plt.plot(arange(0,k+1),xx[:k+1]            ,'k' ,label='true state ($x$)')\n",
    "    plt.plot(arange(1,k+1),yy[:k]              ,'k*',label='noisy obs ($y$)')\n",
    "    \n",
    "    plt.plot(arange(k+1),arange(k+1)*lin_reg(k),'r' ,label='Linear regress.')\n",
    "\n",
    "    #plt.plot(arange(k+1),arange(k+1)*xa [k]/k ,'g--',label='KF extrapolated')\n",
    "    \n",
    "    \n",
    "    ### Uncomment this block AFTER doing the Exc 3.8 ###\n",
    "    #pw_muf, pw_mua = piece_wise_DA_step_lines(xf,xa)\n",
    "    #pw_kkf, pw_kka = piece_wise_DA_step_lines(arange(K+1))    \n",
    "    #plt.plot(pw_kkf[:3*k],pw_muf[:3*k]         ,'c' ,label='KF forecasts')\n",
    "    #plt.plot(pw_kka[:3*k],pw_mua[:3*k]         ,'b' ,label='KF analyses')\n",
    "\n",
    "    \n",
    "    plt.xlim([0,K])\n",
    "    plt.ylim([-1,1.2*a*K])\n",
    "    plt.xlabel('time index (k)')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.6:** Let's reformulate the problem of estimating the parameter $a$   \n",
    "as the problem of estimating $x_k$ (the estimates being related as: $\\hat{a}_k = \\hat{x}_k / k$).  \n",
    "Find a \"forecast model\" $F_k$ (a function of $k$ only) such that the recursion $x_{k+1} = F_k x_k$ is equivalent to eqn (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('LinReg 2 Kalman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KF recursively estimates $x_k$, for $k = 1,2,3,\\ldots$.\n",
    "It repeats a cycle consisting of two steps :\n",
    " \n",
    " * **The forecast step,** which \"propagates\" the estimate:\n",
    " $\\hat{x}_{k}^f = F_{k-1} \\hat{x}_{k-1}^a$.  \n",
    " It also simultaneously keeps track of its current uncertainty (variance):\n",
    " $P_k^f = F_{k-1}^2 P_{k-1}^a + Q$.\n",
    " \n",
    " * **The analysis step,** which \"updates\" the prior/forecast into the posterior/analysis.  \n",
    " This is given by [eqns (8) and (9) of the previous tutorial](T2%20-%20Bayesian%20inference.ipynb#Exc--2.18-'Gaussian-Bayes':), with the symbolic translations:\n",
    "\n",
    "|      |                 |                 |         |         |\n",
    "| ---  | ---             | ---             | ---     | ---     |\n",
    "| T2:  | $b$             | $\\mu$           | $B$     | $P$     |\n",
    "| T3:  | $\\hat{x}_{k}^f$ | $\\hat{x}_{k}^a$ | $P_k^f$ | $P_k^a$ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.8:** Implement the KF to estimate $x_k$ for $k=1,\\ldots, K$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xa  = np.zeros(K+1) # mean  estimates (mu) -- analysis values (a)\n",
    "xf  = np.zeros(K+1) # mean  estimates (mu) -- forecast values (f)\n",
    "PPa = np.zeros(K+1) # covar estimates (P)  -- analysis values (a)\n",
    "PPf = np.zeros(K+1) # covar estimates (P)  -- forecast values (f)\n",
    "PPa[0] = np.inf     # Set initial uncertainty to infinity\n",
    "\n",
    "def F(k):\n",
    "    \"An implmentation of the forecast model F_k\"\n",
    "    if k==0:\n",
    "        # indeterminate and inconsequential\n",
    "        return 1\n",
    "    else:\n",
    "        # Such that xx[k+1] = a*(k+1) = F(k)*xx[k]\n",
    "        return (k+1)/k\n",
    "\n",
    "def KF(k):\n",
    "    \"A cycle of the Kalman filter\"\n",
    "    # Forecast\n",
    "    xf [k+1] = ### INSERT ANSWER HERE ###\n",
    "    PPf[k+1] = ### INSERT ANSWER HERE ###\n",
    "    # Analysis\n",
    "    PPa[k+1] = ### INSERT ANSWER HERE ###\n",
    "    xa [k+1] = ### INSERT ANSWER HERE ###\n",
    "\n",
    "# Run estimations/computations\n",
    "for k in range(K): KF(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('KF_k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may seem more complicated than linear regression. But this \"heavy machinery\" is more general/flexible, and will pay off later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.10:** Go back to the animation above and uncomment the block that plots the KF estimates.\n",
    "Visually: what is the relationship between the estimates provided by the KF and by linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('LinReg compare')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.12*:** This excercise is to prove (on paper) the previous excercise.  \n",
    "The KF forecast step (here with $Q=0$) can be \"baked into\" the analysis step (making the 'a' superscript superfluous), forming a single couple of equations, which are recursive:\n",
    "\n",
    " * $\\hat{x}_k = P_k \\big(y_k/R + F_{k-1} \\hat{x}_{k-1} / [F_{k-1}^2 P_{k-1}] \\big) \\qquad (2)$  \n",
    " * $P_k = 1/\\big(1/R + 1/[F_{k-1}^2 P_{k-1}]\\big) \\qquad \\qquad \\quad (3)$\n",
    "\n",
    "Now:\n",
    "\n",
    "* (a). First show that $P_K = R\\frac{K^2}{\\sum_{k=1}^K k^2} \\, . \\qquad \\quad (4)$\n",
    "* (b). Then show that $\\hat{x}_K = K\\frac{\\sum_{k=1}^K k y_k}{\\sum_{k=1}^K k^2} = K \\hat{a}_K$, where $\\hat{a}_K$ is given by eqn (6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#show_answer('KF == LinReg a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exc 3.14 'Asymptotic P':\n",
    "Consider the scalar KF equations again. Suppose $Q=0$ and that the forward model is $x_{k+1} = F x_k$, with $F>1$. Also suppose that $R_k$ is constant. What does the sequence of $P_k$ converge to? Hint: start from eqn (2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('Asymptotic P')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.16**: Have a look at the code from `show_answer('KF func')`. Comment out the \"weighted average\" form of the update, and try out the \"Kalman gain\" form. Why does it fail (mysterious hint: improper priors)? Can you fix it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('KG fail')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.18:** Now set $Q$ to 1 or more. What happens to the KF estimates? If you want to use the same set of observations, avoid re-executing the cell that simulated/generated them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.20:** Now change $R$. The KF estimates should not change (in this particular example). Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A higher-order example\n",
    "Ok, so the KF produces reasonable results for straight lines (in so far as linear regression does!).\n",
    "What about more intricate time series?\n",
    "\n",
    "Note that the straight line (eqn 1 at the top) could result from discretizing the model\n",
    "\\begin{align*}\n",
    "\\frac{dx}{dt} &= a \\, , \\\\\n",
    "x_0 &= 0 \\, ,\n",
    "\\end{align*}\n",
    "using `dt = 1`.\n",
    "Now, instead, we're going to consider the model\n",
    "$$ \\frac{d^M x}{dt^M} = 0 \\, .$$\n",
    "\n",
    "This can be written as 1-st order vector (i.e. coupled system of) ODE:\n",
    "$$ \\frac{d x^m}{dt} = x^{m+1} \\, , \\quad \\frac{d x^M}{dt} = 0 \\, ,$$\n",
    "where the superscript $m = 1,\\ldots,M$ is the index of the state vector element.\n",
    "\n",
    "To make it more interesting, we'll add two terms to this evolution model:  \n",
    " - damping: $\\beta x^m$, with $\\beta < 0$;\n",
    " - noise: $\\frac{d w^m}{dt}$.  \n",
    "\n",
    "Thus,\n",
    "$$ \\frac{d x^m}{dt} = \\beta x^m + x^{m+1} + \\frac{d w^m}{dt} \\, ,$$\n",
    "where $w^m$ is the noise process, and $\\beta = \\log(0.9)$.\n",
    "\n",
    "Discretized, with a time step `dt=1`, this yields\n",
    "$$ x^m_{k+1} = 0.9 x^m_k + x^{m+1}_k + w^m_k\\, ,$$\n",
    "\n",
    "In summary, $\\mathbf{x}_{k+1} = \\mathbf{F} \\mathbf{x}_k$, with $\\mathbf{F}$ as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M = 4 # model order (and also ndim)\n",
    "F_matrix = 0.9*eye(M) + diag(ones(M-1),1)\n",
    "F_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a full-fledged Kalman filtering formulation of a problem on our hands. \n",
    "\n",
    "We shall not write the code for a multivariate Kalman filter. It is already in DAPPER under `[DAPPER-path]/da_methods.py` and is called `ExtKF()`. The following sets up an experiment with a synthetic truth realization of the model, along with noisy observations. For now, don't worry about the specifics. We'll get back to how to make setups later.\n",
    "\n",
    "We'll only observe the first (0th) component, so that the time series can also be analysed by simple signal processing methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Forecast dynamics\n",
    "f = linear_model_setup(F_matrix)\n",
    "f['noise'] = 0.0001*arange(M)\n",
    "\n",
    "# Initial conditions\n",
    "X0 = GaussRV(m=M,C=0.02*arange(M))\n",
    "\n",
    "# observe 0th component only\n",
    "h = partial_direct_obs_setup(M,[0])\n",
    "h['noise'] = 1000\n",
    "\n",
    "# Time settings\n",
    "t = Chronology(dt=1,dtObs=5,K=250)\n",
    "\n",
    "# Wrap-up\n",
    "HMM = HiddenMarkovModel(f,h,t,X0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generates (simulates) a synthetic truth (xx) and observations (yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xx,yy = simulate(HMM,desc=\"Simulate\")\n",
    "#plt.plot(xx[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll run assimilation methods on the data. The `ExtRTS` method refers to a smoother. It is based on the filter, but also goes back in time, \"smoothing\" out the jumps by assimilating future (relatively speaking) observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stats_KF = ExtKF (store_u=1).assimilate(HMM,xx,yy)\n",
    "stats_KS = ExtRTS(store_u=1).assimilate(HMM,xx,yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following methods perform \"time series analysis\" of the observations, and are mainly derived from signal processing theory.\n",
    "Considering that derivatives can be approximated by differentials, it is plausible that the above model could also be written as an AR(M) process. Thus these methods should perform quite well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.signal as sp_sp\n",
    "normalize = lambda x: x / x.sum()\n",
    "truncate  = lambda x,n: np.hstack([x[:n],zeros(len(x)-n)])\n",
    "TS = {}\n",
    "\n",
    "signal = yy[:,0]\n",
    "\n",
    "TS['Gaussian'] = sp_sp.convolve(signal, normalize(sp.signal.gaussian(30,3)),'same')\n",
    "TS['Wiener']   = sp_sp.wiener(signal)\n",
    "TS['Butter']   = sp_sp.filtfilt(*sp_sp.butter(10, 0.12), signal, padlen=len(signal)//10)\n",
    "TS['Spline']   = sp.interpolate.InterpolatedUnivariateSpline(t.kkObs,signal)(t.kk)\n",
    "TS['Fourier']  = np.fft.irfft(truncate(np.fft.rfft(signal),len(signal)//14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code plots the results. (The GUI is buggy. It might be necessary to execute the cell multiple times.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "@interact(Visible=SelectMultiple(options=['Truth',\n",
    "  'Kalman smoother','Kalman filter','Butter','Gaussian','Wiener','Spline','Fourier']))\n",
    "def plot_results(Visible):\n",
    "    plt.figure(figsize=(9,5))\n",
    "    plt.plot(t.kkObs,yy,'k*',label=\"Obs\")\n",
    "    if 'Truth'           in Visible: plt.plot(t.kk   ,xx[:,0]           ,'k',label=\"Truth\")\n",
    "    if 'Butter'          in Visible: plt.plot(t.kkObs,TS['Butter']      ,'r',label='Butter')\n",
    "    if 'Gaussian'        in Visible: plt.plot(t.kkObs,TS['Gaussian']    ,'g',label='Gaussian')\n",
    "    if 'Wiener'          in Visible: plt.plot(t.kkObs,TS['Wiener']      ,'y',label='Wiener')\n",
    "    if 'Spline'          in Visible: plt.plot(t.kk   ,TS['Spline']      ,'b',label='Spline')\n",
    "    if 'Fourier'         in Visible: plt.plot(t.kkObs,TS['Fourier']     ,'b',label='Trunc. Fourier')\n",
    "    if 'Kalman smoother' in Visible: plt.plot(t.kk   ,stats_KS.mu.u[:,0],'m',label=\"K. smoother\")\n",
    "    if 'Kalman filter'   in Visible:\n",
    "        #plt.plot(t.kkObs,stats_KF.mu.f[:,0],'b',label=\"K. filter (f)\")\n",
    "        #plt.plot(t.kkObs,stats_KF.mu.a[:,0],'c',label=\"K. filter (a)\")\n",
    "        #plt.plot(t.kk   ,stats_KF.mu.u[:,0],'c',label=\"K. filter\")\n",
    "        pw_muf, pw_mua = piece_wise_DA_step_lines(stats_KF.mu.f[:,0],stats_KF.mu.a[:,0])\n",
    "        pw_kkf, pw_kka = piece_wise_DA_step_lines(t.kkObs)\n",
    "        plt.plot(pw_kkf,pw_muf,'b',label=\"KF. forecast\")\n",
    "        plt.plot(pw_kka,pw_mua,'c',label=\"KF. analyses\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually, it's hard to imagine better performance than from the Kalman smoother.\n",
    "However, recall the advantage of the Kalman filter (and smoother): they know the forecast model that generates the truth; they also know the noise levels Q and R (but they don't know the actual outcomes/realizations of the random noises), which also means that they do not need any *tuning*, or having to choose between the myriad of signal processing filters [out there](https://docs.scipy.org/doc/scipy/reference/signal.html#module-scipy.signal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def average_error(estimate_at_obs_times):\n",
    "    return np.mean(np.abs(xx[t.kkObs,0] - estimate_at_obs_times))\n",
    "\n",
    "print('Gaussian'   , average_error(TS['Gaussian']))\n",
    "print('Wiener'     , average_error(TS['Wiener']))\n",
    "print('Butter'     , average_error(TS['Butter']))\n",
    "print('Spline'     , average_error(TS['Spline'][t.kkObs]))\n",
    "print('Fourier'    , average_error(TS['Fourier']))\n",
    "print('K. smoother', average_error(stats_KS.mu.u[t.kkObs,0]))\n",
    "print('K. filter'  , average_error(stats_KF.mu.a[:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.22:** Theoretically, in the long run, the Kalman smoother should yield the best result. Verify this by increasing the experiment length to K=10**4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.24:** Re-run the experiment with different paramters, for example the observation noise strength or `dkObs`.  \n",
    "[Results will differ even if you changed nothing because the truth noises (and obs) are stochastic.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.26:** Right before executing the assimilations (but after simulating the truth and obs), change $R$ by inserting:\n",
    "\n",
    "    HMM.h.noise = GaussRV(C=0.01*eye(1))\n",
    "    \n",
    "What happens to the estimates of the Kalman filter and smoother?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Exc 3.28*:** Try out different methods from DAPPER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\"Time series analysis\", which is what we've been seeing so far, is a subset of \"Data assimilation\" (i.e. state estimation) [(much of time series analysis can be formulated as state estimation)](https://www.google.com/search?q=\"We+now+demonstrate+how+to+put+these+models+into+state+space+form\"). Moreover, DA methods also yield uncertainty quantifications, something which is usually more obscure with \"time series analysis\" methods.\n",
    "\n",
    "DA really shines in the multivariate case. In part by its capacity to deal with sparsely observed systems, and in part because multivariate systems are often chaotic (which is more fun than stochastically-driven signals such as the above example). This is what we'll get to next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next: [Dynamical systems, chaos, Lorenz](T4 - Dynamical systems, chaos, Lorenz.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "6371b235e56649288368b01da3129569": {
     "views": [
      {
       "cell_index": 39
      }
     ]
    },
    "6a4b36c5fc8d401ba6244926c8760819": {
     "views": [
      {
       "cell_index": 39
      }
     ]
    },
    "838ce3dd868e4b25b563e7637536934a": {
     "views": [
      {
       "cell_index": 39
      }
     ]
    },
    "916f5cd483c14ffbb4829c8c4e4b1008": {
     "views": [
      {
       "cell_index": 39
      }
     ]
    },
    "a90c0ee6ca8b4702bc7701fc2e6d8cf7": {
     "views": [
      {
       "cell_index": 39
      }
     ]
    },
    "ffe9d79f53914dc08f21a895d7745483": {
     "views": [
      {
       "cell_index": 39
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
