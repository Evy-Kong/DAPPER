{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from resources.workspace import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The (univariate) Kalman filter (KF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KF estimates the state variable (truth) $x$,  \n",
    "which is assumed to \"evolve\" according to some linear \"dynamics\":\n",
    "$$ x_{k} = F_{k-1} x_{k-1} + q_{k-1} \\, , \\qquad (\\text{Dyn}) $$\n",
    "where $q_{k-1} \\sim \\mathcal{N}(0, Q)$ is known dynamic process noise.  \n",
    "Here, $k = 1,2,3,\\ldots$ denotes the time indices.\n",
    "\n",
    "The KF recursively (for increasing $k$) estimates $x_k$ .\n",
    "It repeats a cycle consisting of two steps :\n",
    " \n",
    "#### The forecast step\n",
    "\"propagates\" the estimate (the pdf) of $x$.  \n",
    "Note that (it may be shown that):  \n",
    "if $x \\sim \\mathcal{N}(\\mu, P)$ and $q \\sim \\mathcal{N}(0, Q)$  \n",
    "then $F x + q \\;\\sim\\; \\mathcal{N}(F \\mu, \\; F^2P + Q)$  \n",
    "\n",
    "\n",
    "Therefore, since $x_k$ obeys (Dyn),  \n",
    "   if $\\; p(x_{k-1}) = \\mathcal{N}(x_{k-1} \\mid \\; \\phantom{F_{k-1}}\\hat{x}_{k-1}^a,\\; \\phantom{F_{k-1}^2} P_{k-1}^a) \\, ,$\n",
    "   for some $\\hat{x}_{k-1}^a, \\, P_{k-1}^a$,   \n",
    "   then  $p(x_k)     = \\mathcal{N}(x_k \\; \\; \\, \\mid \\;  \\underbrace{F_{k-1}\n",
    "   \\hat{x}_{k-1}^a}_{{\\hat{x}_{k}}^f},\\; \\underbrace{F_{k-1}^2 P_{k-1}^a + Q}_{{P_k}^f}) \\, .$  \n",
    "   The KF forecast step amounts to the computation of the moments with superscript $f$. \n",
    "\n",
    " \n",
    "#### The analysis step\n",
    "\"updates\" the prior/forecast into the posterior/\"analysis\", based on $p(y_k|x_k) = \\mathcal{N}(y_k \\mid x_k,R)$.  \n",
    " It was derived in [the previous tutorial](T2%20-%20Bayesian%20inference.ipynb#Gaussian-Gaussian-Bayes), with the symbolic translations in the below table. The KF analysis step amounts to the computation of the moments with superscript $a$. \n",
    " \n",
    "|         |                 |                 |         |         |\n",
    "| ---     | ---             | ---             | ---     | ---     |\n",
    "| Tut 2:  | $b$             | $B$             | $\\mu$   | $P$     |\n",
    "| Tut 3:  | $\\hat{x}_{k}^f$ | $P_k^f$ | $\\hat{x}_{k}^a$ | $P_k^a$ |\n",
    "\n",
    "   \n",
    "\n",
    "Note that this completes the cycle, which can then restart with the forecast from $k$ to $k+1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A straight-line example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many mathematical methods are tagged as \"least squares\". They typically have one thing in common: some sum of squared terms is being minimized. Both (least-squares) linear regression and Kalman filtering (KF) may be derived from \"least squares\".\n",
    "Do they yield the same estimate (when applied to the linear regression problem)? That's what we'll investigate...\n",
    "\n",
    "Consider the straight line\n",
    "$$x_k = a k \\, ,     \\qquad \\qquad (1) $$\n",
    "and suppose we have observations ($y$) of the line, but corrupted by noise ($r$):\n",
    "\\begin{align*}\n",
    "y_k &= x_k + r_k \\, , \\;\\; \\qquad (2)\n",
    "\\end{align*}\n",
    "where $r_k \\sim \\mathcal{N}(0, R)$ for some $R>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below sets up an experiment based on eqns. (1) and (2).  \n",
    "Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = 0.4  # Slope (xx[k] = a*k) paramterer. Unknown to be estimated.\n",
    "K = 10   # Length of experiment (final time index)\n",
    "R = 1    # Observation noise strength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following simulates a series of the truth ($x$) and observations ($y$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Recall the naming convention: xx and yy hold time series of x and y.\n",
    "xx = np.zeros(K+1) # truth states\n",
    "yy = np.zeros(K+1) # obs\n",
    "\n",
    "for k in 1+arange(K):\n",
    "    xx[k] = a*k\n",
    "    yy[k] = xx[k] + sqrt(R)*randn()\n",
    "\n",
    "# The obs at k==0 should not be used (since we know xx[0]==0, it is worthless).\n",
    "yy[0] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esitmation by linear regression\n",
    "(Least-squares) linear regression minimizes\n",
    "$$J_K(\\hat{a}) = \\sum_{k=1}^K (y_k - \\hat{a} k)^2 \\, ,  \\qquad (4)$$\n",
    "yielding the estimator\n",
    "$$\\hat{a}_K = \\frac{\\sum_{k=1}^K {k} y_{k}}{\\sum_{k=1}^K {k}^2} \\, . \\qquad \\qquad (6)$$\n",
    "\n",
    "**Exc 3.2:** Derive (6) from (4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('LinReg deriv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.4:** Program the linear regresson estimator (6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lin_reg(k):\n",
    "    \"Liner regression estimator based on observations y_1, ..., y_k.\"\n",
    "    ### INSERT ANSWER HERE ###\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('LinReg_k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@interact(k=IntSlider(min=1, max=K))\n",
    "def plot_experiment(k):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    kk = arange(k+1)\n",
    "    plt.plot(kk,xx[kk]        ,'k' ,label='true state ($x$)')\n",
    "    plt.plot(kk,yy[kk]        ,'k*',label='noisy obs ($y$)')\n",
    "    plt.plot(kk,kk*lin_reg(k) ,'r' ,label='Linear regress.')\n",
    "\n",
    "    ### Uncomment this block AFTER doing the Exc 3.8 ###\n",
    "    # pw_xxf, pw_xxa = weave_fa(xxf,xxa)\n",
    "    # pw_kkf, pw_kka = weave_fa(arange(K+1))    \n",
    "    # plt.plot(pw_kkf[:3*k],pw_xxf[:3*k] ,'c'  ,label='KF forecasts')\n",
    "    # plt.plot(pw_kka[:3*k],pw_xxa[:3*k] ,'b'  ,label='KF analyses')\n",
    "    # #plt.plot(kk,kk*xa[k]/k            ,'g--',label='KF extrapolated')\n",
    "\n",
    "    plt.xlim([0,1.01*K])\n",
    "    plt.ylim([-1,1.2*a*K])\n",
    "    plt.xlabel('time index (k)')\n",
    "    plt.ylabel('$x$, $y$, and $\\hat{x}$')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Estimation by the KF\n",
    "In the following we tacke the same problem, but using the KF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observations equation (2)\n",
    "yields the likelihood $p(y_k|x_k) = \\mathcal{N}(y_k \\mid x_k,R)$.  \n",
    "Hopefully this is intuitive; otherwise, a more detailed derivation will be given in tutorial 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.6:** For the (univariate) KF,\n",
    "we need to reformulate the problem of estimating the parameter $a$ \n",
    "as the problem of estimating $x_k$ (yielding $\\hat{a}_k = \\hat{x}_k / k$).\n",
    "Derive and implement the \"forecast model\" $F_k$ (a function of $k>0$ only) such that\n",
    "the recursion $x_{k+1} = F_k x_k$ is equivalent to eqn (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def F(k):\n",
    "    return ### INSERT ANSWER HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('Sequential 2 Recusive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.7:** Explain why the KF is applicable to a larger class of problems than linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('LinReg âŠ‚ KF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So although the KF (and its implementation below) may seem like \"overkill\" for this problem,\n",
    "this \"heavy machinery\" can do a lot more, and will pay off later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.8:** Implement the KF (described at the top of this notebook) in the below code block to estimate $x_k$ for $k=1,\\ldots, K$.  \n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<b>NB:</b> for this example, do not use the \"Kalman gain\" form of the analysis update.\n",
    "This problem involves the peculiar, unrealistic situation of infinities\n",
    "(related to \"improper priors\") at `k==1`, yielding platform-dependent behaviour.\n",
    "These peculariaties are of mainly of academic interest, and is not our focus here.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Q = 0 # Dynamical model noise strength\n",
    "\n",
    "# Allocation\n",
    "xxa = np.zeros(K+1) # mean estimates -- analysis values (a)\n",
    "xxf = np.zeros(K+1) # mean estimates -- forecast values (f)\n",
    "PPa = np.zeros(K+1) # var  estimates -- analysis values (a)\n",
    "PPf = np.zeros(K+1) # var  estimates -- forecast values (f)\n",
    "\n",
    "def KF(k):\n",
    "    \"A cycle of the Kalman filter\"\n",
    "    # Forecast\n",
    "    if k==1:\n",
    "        PPf[k] = np.inf # The \"initial\" prior uncertainty is infinite...\n",
    "        xxf[k] = 0      # ... thus the corresponding mean is inconsequential.\n",
    "    else:\n",
    "        PPf[k] = ### INSERT ANSWER HERE ###\n",
    "        xxf[k] = ### INSERT ANSWER HERE ###\n",
    "    # Analysis\n",
    "    PPa[k] = ### INSERT ANSWER HERE ###\n",
    "    xxa[k] = ### INSERT ANSWER HERE ###\n",
    "\n",
    "# Run estimations/computations\n",
    "for k in 1+arange(K):\n",
    "    KF(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('KF_k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.10:** Go back to the animation above and uncomment the block that plots the KF estimates.  \n",
    "Visually: what is the relationship between the estimates provided by the KF and by linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('LinReg compare')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em>Exercises marked with an asterisk (*) are optional.</em>\n",
    "\n",
    "**Exc 3.12*:** This excercise proves (on paper) the result of the previous excercise.  \n",
    "Note that the KF forecast step (here with $Q=0$) can be inserted in the analysis step, forming a single couple of recursions (and making the 'a' superscript unnecessary), which are recursive:\n",
    "\n",
    " * $\\hat{x}_k = P_k \\big(y_k/R \\;+\\; F_{k-1} \\hat{x}_{k-1} / [F_{k-1}^2 P_{k-1}] \\big) \\qquad (11)$  \n",
    " * $P_k = 1/\\big(1/R \\;+\\; 1/[F_{k-1}^2 P_{k-1}]\\big) \\qquad \\qquad \\quad (12)$\n",
    "\n",
    "Now:\n",
    "\n",
    "* (a). First show that $P_K = R\\frac{K^2}{\\sum_{k=1}^K k^2} \\, . \\;\\;\\; \\qquad \\quad (13)$\n",
    "* (b). Then show that $\\hat{x}_K = K\\frac{\\sum_{k=1}^K k y_k}{\\sum_{k=1}^K k^2} = K \\hat{a}_K$, where $\\hat{a}_K$ is given by eqn (11)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#show_answer('x_KF == x_LinReg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exc 3.14:\n",
    "Let $x_{k+1} = F x_k$, *for a generic $F>1$ (note: $Q=0$)*. What does the sequence of $P_k$ converge to?  \n",
    "You must start from eqn (12), because eqn (13) is for the straight-line example only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('Asymptotic P when F>1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exc 3.15:\n",
    "Redo Exc 3.14, but assuming  \n",
    " * (a) $F = 1$.\n",
    " * (b) $F < 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('Asymptotic P when F=1')\n",
    "#show_answer('Asymptotic P when F<1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, if $F>1$, the KF does not converge to 0 error. This is because, even though you keep gaining more information, this is balanced by the growth in uncertainty by the forecast. On the other hand, if $F \\leq 1$ then the error converges to zero.\n",
    "\n",
    "In general, however, $F$ depends on $k$, as will the other system matrices ($Q, R$).\n",
    "Then, there is no limit value that the state distribution (and its parameters) converges to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.18*:** Set $Q$ to 1 or more. Re-compute the KF estimates. By interpreting the meaning of $Q$, explain why the KF estimate is now closer to the obs (always at the latest time instance) than the linear regression estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.20*:** Now change $R$. The KF estimates should not change (in this particular example). Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# A higher-order example\n",
    "Above we saw that the KF produces reasonable results for straight lines (in so far as linear regression does!).\n",
    "What about more intricate time series?\n",
    "\n",
    "### The model\n",
    "Note that the straight line (eqn 1 at the top) could result from discretizing the model\n",
    "\\begin{align*}\n",
    "\\frac{dx}{dt} &= a \\, , \\\\\n",
    "x(0) &= 0 \\, ,\n",
    "\\end{align*}\n",
    "using `dt = 1`.\n",
    "Now, instead, we're going to consider the model\n",
    "$$ \\frac{d^M x}{dt^M} = 0 \\, .$$\n",
    "\n",
    "This can be written as 1-st order vector (i.e. coupled system of) ODE:\n",
    "$$ \\frac{d x^m}{dt} = x^{m+1} \\, , \\quad \\frac{d x^M}{dt} = 0 \\, ,$$\n",
    "where the superscript $m = 1,\\ldots,M$ is the index of the state vector element.\n",
    "\n",
    "To make it more interesting, we'll add two terms to this evolution model:  \n",
    " - damping: $\\beta x^m$, with $\\beta < 0$;\n",
    " - noise: $\\frac{d q^m}{dt}$.  \n",
    "\n",
    "Thus,\n",
    "$$ \\frac{d x^m}{dt} = \\beta x^m + x^{m+1} + \\frac{d q^m}{dt} \\, ,$$\n",
    "where $q^m$ is the noise process, and $\\beta = \\log(0.9)$.\n",
    "\n",
    "Discretized, with a time step `dt=1`, this yields\n",
    "$$ x^m_{k+1} = 0.9 x^m_k + x^{m+1}_k + q^m_k\\, ,$$\n",
    "\n",
    "In summary, $\\mathbf{x}_{k+1} = \\mathbf{F} \\mathbf{x}_k + \\mathbf{q}_k$, with $\\mathbf{F}$ as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M = 4 # model order (and also ndim)\n",
    "F_matrix = 0.9*eye(M) + diag(ones(M-1),1)\n",
    "print(F_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation by the Kalman filter (and smoother) with DAPPER\n",
    "\n",
    "Note that this is an $M$-dimensional time series. \n",
    "However, we'll only observe the first (0th) component.\n",
    "\n",
    "We shall not write the code for the multivariate Kalman filter,\n",
    "because it already exists in DAPPER in `da_methods.py` and is called `ExtKF()`.\n",
    "\n",
    "The following code configures an experiment based on the above model. Don't worry about the specifics. We'll get back to how to use DAPPER later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Forecast dynamics\n",
    "Dyn = linear_model_setup(F_matrix)\n",
    "Dyn['noise'] = 0.0001*(1+arange(M))\n",
    "\n",
    "# Initial conditions\n",
    "X0 = GaussRV(m=M,C=0.02*arange(M))\n",
    "\n",
    "# Observe 0th component only\n",
    "Obs = partial_direct_obs_setup(M,[0])\n",
    "Obs['noise'] = 1000\n",
    "\n",
    "# Time settings\n",
    "t = Chronology(dt=1,dtObs=5,K=250)\n",
    "\n",
    "# Wrap-up\n",
    "HMM = HiddenMarkovModel(Dyn,Obs,t,X0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generates (simulates) a synthetic truth (xx) and observations (yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xx,yy = simulate(HMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "for m,x in enumerate(xx.T):\n",
    "    plt.plot(x,label=\"x^%d\"%m)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll run assimilation methods on the data. Firstly, the KF, available as `ExtKF` in DAPPER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stats_KF = ExtKF (store_u=1).assimilate(HMM,xx,yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also run the \"Kalman smoother\" available as `ExtRTS`.\n",
    "Without going into details, this method is based on the Kalman *filter* but,\n",
    "being a *smoother*,\n",
    "it also goes backwards and updates previous estimates with future (relatively speaking) observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stats_KS = ExtRTS(store_u=1).assimilate(HMM,xx,yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation by \"time series analysis\"\n",
    "The following methods perform time series analysis of the observations, and are mainly derived from signal processing theory.\n",
    "Considering that derivatives can be approximated by differentials, it is plausible that the above model could also be written as an AR(M) process. Thus these methods should perform quite well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tools\n",
    "import scipy.signal as sp_sp\n",
    "normalize = lambda x: x / x.sum()\n",
    "truncate  = lambda x,n: np.hstack([x[:n],zeros(len(x)-n)])\n",
    "\n",
    "# We only estimate the 0-th component.\n",
    "signal = yy[:,0]\n",
    "\n",
    "# Estimated signals\n",
    "ESig = {} \n",
    "ESig['Gaussian']       = sp_sp.convolve(signal, normalize(sp.signal.gaussian(30,3)),'same')\n",
    "ESig['Wiener']         = sp_sp.wiener(signal)\n",
    "ESig['Butter']         = sp_sp.filtfilt(*sp_sp.butter(10, 0.12), signal, padlen=len(signal)//10)\n",
    "ESig['Spline']         = sp.interpolate.UnivariateSpline(t.kkObs,signal,s=1e4)(t.kkObs)\n",
    "ESig['Trunc. Fourier'] = np.fft.irfft(truncate(np.fft.rfft(signal),len(signal)//14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison\n",
    "The following code plots the results.\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<b>NB:</b> The GUI is buggy; it might be necessary to execute the cell multiple times. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "@interact(Visible=SelectMultiple(options=['Truth','Kalman smoother','Kalman filter','My Method']+list(ESig)))\n",
    "def plot_results(Visible):\n",
    "    plt.figure(figsize=(9,5))\n",
    "    plt.plot(t.kkObs,yy,'k.',alpha=0.4,label=\"Obs\")\n",
    "    if 'Truth'           in Visible: plt.plot(t.kk   ,xx[:,0]           ,'k',label=\"Truth\")\n",
    "    if 'Kalman smoother' in Visible: plt.plot(t.kk   ,stats_KS.mu.u[:,0],'m',label=\"K. smoother\")\n",
    "    #if'Kalman filter u' in Visible: plt.plot(t.kk   ,stats_KF.mu.u[:,0],'b',label=\"K. filter (u)\")\n",
    "    #if'Kalman filter f' in Visible: plt.plot(t.kkObs,stats_KF.mu.f[:,0],'b',label=\"K. filter (f)\")\n",
    "    #if'Kalman filter a' in Visible: plt.plot(t.kkObs,stats_KF.mu.a[:,0],'b',label=\"K. filter (a)\")\n",
    "    if 'Kalman filter'   in Visible:\n",
    "        pw_xxf, pw_xxa = weave_fa(stats_KF.mu.f[:,0],stats_KF.mu.a[:,0])\n",
    "        pw_kkf, pw_kka = weave_fa(t.kkObs)\n",
    "        plt.plot(pw_kkf,pw_xxf,'b',label=\"KF. forecast\")\n",
    "        plt.plot(pw_kka,pw_xxa,'c',label=\"KF. analysis\")\n",
    "    \n",
    "    if 'My Method' in Visible and 'stats_MM' in locals():\n",
    "        pw_xxf, pw_xxa = weave_fa(stats_MM.mu.f[:,0],stats_MM.mu.a[:,0])\n",
    "        pw_kkf, pw_kka = weave_fa(t.kkObs)\n",
    "        plt.plot(pw_kkf,pw_xxf,'y',label=stats_MM.config.da_method.__name__+\" forecast\")\n",
    "        plt.plot(pw_kka,pw_xxa,'g',label=stats_MM.config.da_method.__name__+\" analysis\")\n",
    "    \n",
    "    for method, estimate in ESig.items():\n",
    "        if method in Visible: plt.plot(t.kkObs, estimate,label=method)\n",
    "    \n",
    "    plt.ylabel('$x^0$, $y$, and $\\hat{x}^0$')\n",
    "    plt.xlabel('Time index ($k$)')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually, it's hard to imagine better performance than from the Kalman smoother.\n",
    "However, recall the advantage of the Kalman filter (and smoother): *they know the forecast model that generated the truth*.\n",
    "\n",
    "Since the noise levels Q and R are given to the DA methods (but they don't know the actual outcomes/realizations of the random noises), they also do not need any *tuning*, compared to signal processing filters, or choosing between the myriad of signal processing filters [out there](https://docs.scipy.org/doc/scipy/reference/signal.html#module-scipy.signal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def average_error(estimate_at_obs_times):\n",
    "    return np.mean(np.abs(xx[t.kkObs,0] - estimate_at_obs_times))\n",
    "\n",
    "for method, estimate in ESig.items():\n",
    "    print(method   , average_error(estimate))\n",
    "print('K. smoother', average_error(stats_KS.mu.u[t.kkObs,0]))\n",
    "print('K. filter'  , average_error(stats_KF.mu.a[:,0]))\n",
    "# print('My Method', average_error(stats_MM.mu.a[:,0])) # uncomment after Exc 3.28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.22:** Theoretically, in the long run, the Kalman smoother should yield the optimal result. Verify this by increasing the experiment length to K=10**4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.24:** Re-run the experiment with different paramters, for example the observation noise strength or `dkObs`.  \n",
    "[Results will differ even if you changed nothing because the truth noises (and obs) are stochastic.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.26:** Right before executing the assimilations (but after simulating the truth and obs), change $R$ by inserting:\n",
    "\n",
    "    HMM.h.noise = GaussRV(C=0.01*eye(1))\n",
    "    \n",
    "What happens to the estimates of the Kalman filter and smoother?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Exc 3.28*:** Try out different methods from DAPPER by replacing `MyMethod` below with one of the following:\n",
    " - Climatology\n",
    " - Var3D\n",
    " - OptInterp\n",
    " - EnKF\n",
    " - EnKS\n",
    " - PartFilt\n",
    "\n",
    "You typically also need to set (and possibly tune) some method parameters. Otherwise you will get an error (or possibly the method will perform very badly). You may find (some) documentation for each method in its source code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stats_MM = MyMethod(param1=val1,...).assimilate(HMM,xx,yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "Like linear regression, time series analysis is also a subset of  data assimilation [(much of time series analysis can be formulated as state estimation)](https://www.google.com/search?q=\"We+now+demonstrate+how+to+put+these+models+into+state+space+form\"). Moreover, DA methods produce uncertainty quantification, something which is usually more obscure with time series analysis methods.\n",
    "\n",
    "Still, the best is yet to come: DA methods should have the capacity to handle inhomogeneous, multivariate, sparsely observed, chaotic systems (which is more fun than stochastically-driven signals such as the above example). This is what we'll get to next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next: [Dynamical systems, chaos, Lorenz](T4 - Dynamical systems, chaos, Lorenz.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "8e94e3502b694e6e85eee82b64e8bfe4": {
     "views": [
      {
       "cell_index": 48
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
