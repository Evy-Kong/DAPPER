{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from resources.workspace import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Gaussian (i.e. Normal) distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the random variable with a Gaussian distribution with mean $\\mu$ (`mu`) and variance $P$. We write its probability density function (**pdf**) as\n",
    "$$ p(x) = N(x|\\mu,P) = (2 \\pi P)^{-1/2} e^{-(x-\\mu)^2/2P} \\, .  \\qquad \\qquad (G1) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.2:** Code it up (complete the code below)! Hints:\n",
    "* Note that `**` is the power operator in Python.\n",
    "* As in Matlab, $e^x$ is available as `exp(x)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Univariate (scalar), Gaussian pdf\n",
    "def pdf_G1(x,mu,P):\n",
    "    # pdf_values = ### INSERT ANSWER HERE ###\n",
    "    return pdf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('pdf_G1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's plot the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu  = 0                 # mean     of distribution\n",
    "P   = 25                # variance of distribution\n",
    "P12 = sqrt(P)           # std. dev of distribution\n",
    "\n",
    "# Plotting\n",
    "N  = 201                # num of grid points\n",
    "xx = linspace(-20,20,N) # grid\n",
    "dx = xx[1]-xx[0]        # grid spacing\n",
    "pp = pdf_G1(xx,mu,P)   # pdf values\n",
    "plt.subplot(211)        # allocate plot panel\n",
    "plt.plot(xx,pp);        # plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This could for example be the pdf of a stochastic noise variable. It could also describe our quantitative belief and uncertainty about a parameter (or state), which we model as randomness in the Bayesian view on probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.4:** From looking at the figure:\n",
    " * How does the pdf curve change when `mu` changes?  \n",
    "   (alter the above code, and re-run the cell)\n",
    " * How does the pdf curve change when you increase `P`?  \n",
    " \n",
    " \n",
    "Re-set `P=25` and re-run (this is a convienient value for examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.6:** Recall $p(x) = N(x|\\mu,P)$ from eqn (G1).  \n",
    "The following are helpful points to remember how it looks. Use pen, paper, and calculus.  \n",
    "Hint: it's typically easier to analyse $\\log p(x)$ rather than $p(x)$ itself.\n",
    " * Where is the location of the mode (maximum) of the distribution? I.e. where $\\frac{d p}{d x}(x) = 0$.\n",
    " * Where is the inflection point? I.e. where $\\frac{d^2 p(x)}{d x^2} = 0$.\n",
    " * What is the value of $\\frac{d^2 \\log p}{d x^2}(x)$ at the mode?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The multivariate (i.e. vector) case\n",
    "Here's the pdf of the *multivariate* Gaussian:\n",
    "\\begin{align}\n",
    "N(x \\,|\\, \\mu,P) &= |2 \\pi P|^{-1/2} \\, \\exp\\Big(-\\frac{1}{2}\\|x-\\mu\\|^2_P\\Big) \\, , \\qquad \\qquad (GM) \\\\\\\n",
    "\\end{align}\n",
    "where $|.|$ represents the determinant, and $\\|.\\|_W$ represents the norm with weighting: $\\|x\\|^2_W = x^T W^{-1} x$.  \n",
    "The following implements this pdf. Take a moment to digest the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import det, inv\n",
    "\n",
    "def weighted_norm22(xx,W):\n",
    "    \"Computes the norm of each row vector of xx, as weighted by W.\"\n",
    "    return np.sum((xx @ inv(W)) * xx, axis=1)\n",
    "\n",
    "def pdf_GM(xx,mu,P):\n",
    "    \"GaussPDF(x|mu,P). Accepts multiple x vectors at once (as rows of xx).\"\n",
    "    return 1/sqrt(det(2*pi*P))*exp(-0.5*weighted_norm22(xx-mu,P))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code plots the pdf as contour (equi-density) curves.  \n",
    "*The plot appears in the above figure.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Grid\n",
    "def flatten(xxyy): return array([xi.ravel() for xi in xxyy]).T\n",
    "def  square(xyxy): return xyxy.reshape(int(sqrt(len(xyxy))),-1)\n",
    "\n",
    "grid = flatten(np.meshgrid(xx,xx))\n",
    "\n",
    "# Covariance specification\n",
    "corr = 0.7\n",
    "Cov = P * array([[1,corr],\n",
    "                 [corr,1]])\n",
    "# Eval\n",
    "pp = pdf_GM(grid, 0, Cov)\n",
    "pp = square(pp)\n",
    "\n",
    "# Plot\n",
    "plt.subplot(212).clear()\n",
    "plt.contour(xx,xx,pp)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.8:** How do the contours look? Try to understand why. Cases:\n",
    " * (a) correlation=0.    \n",
    " * (b) correlation=0.99.\n",
    " * (c) correlation=0.5. (Note that we've used `plt.axis('equal')`).\n",
    " * (d) correlation=0.5, but with non-equal variances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.9:** Go play the [correlation game](http://guessthecorrelation.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes' rule\n",
    "Bayes' rule is how we do inference. For continuous random variables, $x$ and $y$, it reads:\n",
    "\n",
    "$$ p(x|y) = \\frac{p(x) \\, p(y|x)}{p(y)} \\, , \\qquad \\qquad (2)$$\n",
    "\n",
    "or, in words:\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{\"posterior\" (pdf of $x$ given $y$)}\n",
    "\\; = \\;\n",
    "\\frac{\\text{\"prior\" (pdf of $x$)}\n",
    "\\; \\times \\;\n",
    "\\text{\"likelihood\" (pdf of $y$ given $x$)}}\n",
    "{\\text{\"normalization\" (pdf of $y$)}}\n",
    "$$.\n",
    "\n",
    "**Exc 2.10:** Derive Bayes' rule from the definition of [conditional pdf's](https://en.wikipedia.org/wiki/Conditional_probability#Kolmogorov_definition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('BR derivation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computers generally work with discrete, numerical representations of mathematical entities.\n",
    "Numerically, pdfs may be represented by their `values` on a grid, such as `xx` from above. Bayes' rule (2) then consists of *grid-point-wise* multiplication, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Bayes_rule(prior_values,lklhd_values,dx):\n",
    "    pp = prior_values * lklhd_values   # pointwise multiplication\n",
    "    posterior_values = pp/(sum(pp)*dx) # normalization\n",
    "    return posterior_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.12:** Why does `Bayes_rule()` not need the denominator, $p(y)$, as input?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('BR grid normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, since normalization is so simple, we often don't bother to do it until it's strictly necessary. Therefore we often simplify Bayes' rule (2) as\n",
    "$$ p(x|y) \\propto p(x) \\, p(y|x) \\, .  \\qquad \\qquad (3) $$\n",
    "\n",
    "However, sometimes (e.g. non-parametric sampling methods), it is not so simple, and the normalization becomes an important question too.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The code below show's Bayes' rule in action.  \n",
    "Again, remember that the only thing it's doing is multiplying the prior and likelihood at each gridpoint.  \n",
    "Move the sliders with the arrow keys to animate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "b = 0 # prior mean\n",
    "B = 1 # prior variance\n",
    "\n",
    "@interact(y=(-10,10,1),log_R=(-2,5,0.5))\n",
    "def animate_Gaussian_Bayes(y=4.0,log_R=1):\n",
    "\n",
    "    R = exp(log_R)\n",
    "\n",
    "    prior     = lambda x: pdf_G1(x,b,B)\n",
    "    lklhd     = lambda x: pdf_G1(y,x,R)\n",
    "    \n",
    "    post_vals = Bayes_rule(prior(xx),lklhd(xx),xx[1]-xx[0])\n",
    "\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(xx,prior(xx)     ,label='prior N(x|0,1)')\n",
    "    plt.plot(xx,lklhd(xx)     ,label='likelihood N(y|x,R)')\n",
    "    plt.plot(xx,post_vals     ,label='posterior - pointwise')\n",
    "    \n",
    "    ### Uncomment this block AFTER doing the exercise ###\n",
    "    ### that defines Bayes_rule_G1()            ###\n",
    "    #mu, P     = Bayes_rule_G1(b,B,y,R)\n",
    "    #postr     = lambda x: pdf_G1(x,mu,P)\n",
    "    #plt.plot(xx,postr(xx),'--',label='posterior - parametric')\n",
    "    \n",
    "    plt.ylim(ymax=0.6)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.14:** Answer the following by moving the sliders and seeing what happens.\n",
    " * What happens to the posterior when $R \\rightarrow \\infty$ ?\n",
    " * What happens to the posterior when $R \\rightarrow 0$ ?\n",
    " * What is the posterior's location (mean/mode) when $R = B$ ? (try moving around $y$)\n",
    " * Does the posterior scale (width) depend on $y$?\n",
    " * Consider the shape (ignoring location & scale) of the posterior. Does it depend on $R$ or $y$?\n",
    " * Can you see a shortcut to computing this posterior rather than having to do the pointwise multiplication?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.15*:** Implement a \"uniform\" (or \"flat\" or \"box\") distribution pdf and call it `pdf_U1(x,mu,P)`. These <a href=\"https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)#Moments\">formulae</a> for its mean/variance will be useful. \n",
    "\n",
    "In the above animations, replace `pdf_G1` with your new `pdf_U1` (both for the prior and likelihood). Assure that everything is working correctly. \n",
    " - Why (in the figure) are the walls of the pdf (ever so slightly) inclined?\n",
    " - What happens when you move the prior and likelihood too far apart? Is the fault of the implementation, or the math, or the assumptions (uniform distribution)?\n",
    " - Re-do Exc 2.14, now with `pdf_U1`.\n",
    " - Now test a Gaussian prior with a uniform likelihood.\n",
    " - Restore `pdf_G1` (both the prior and likelihood) in the animation (for later use).\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('pdf_U1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Gaussian-Gaussian Bayes\n",
    "\n",
    "The above animation shows Bayes' rule in 1 dimension. Previously, we saw how a Gaussian looks in 2 dimensions. Can you imagine how Bayes' rule looks in 2 dimensions? In higher dimensions, these things get difficult to imagine, let alone visualize.\n",
    "\n",
    "Similarly, the size of the calculations required for Bayes' rule poses a difficulty. Indeed, the following exercise shows that (pointwise) multiplication for all grid points becomes a preposterious notion in high dimensions.\n",
    "\n",
    "**Exc 2.16:**\n",
    " * (a) How many point-multiplications are needed on a grid with $N$ points in $M$ dimensions? (Imagine an $M$-dimensional cube where each side has a grid with $N$ points on it)\n",
    " * (b) Suppose we model 15 physical quanitites, on each grid point, on a discretized surface model of Earth. Assume the resolution is $1^\\circ$ for latitude (110km), $1^\\circ$ for longitude. How many variables are there in total? This is the dimensionality ($M$) of the problem.\n",
    " * (c) Suppose each variable is has a pdf represented with a grid using only $N=10$ points. How many multiplications are necessary to calculate Bayes rule (jointly) for all variables on our Earth model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('Dimensionality a')\n",
    "#show_answer('Dimensionality b')\n",
    "#show_answer('Dimensionality c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In response to this computational difficulty, we try to be smart and do something more analytical (\"pen-and-paper\"): we only compute the parameters (mean and (co)variance) of the posterior pdf.\n",
    "\n",
    "This is doable and quite simple in the Gaussian-Gaussian case.\n",
    "With a prior $p(x) = N(x\\,|\\,b,B)$ and a likelihood $p(y|x) = N(y\\,|\\,x,R)$, the posterior will be given by\n",
    "\\begin{align}\n",
    "p(x|y)\n",
    "&= N(x|\\mu,P) \\qquad \\qquad (4) \n",
    "\\, ,\n",
    "\\end{align}\n",
    "where, in the univarite (1-dimensional) case:\n",
    "\\begin{align}\n",
    "    P &= 1/(1/B + 1/R) \\, , \\qquad \\qquad (5) \\\\\\\n",
    "  \\mu &= P(b/B + y/R) \\, .  \\qquad \\qquad (6) \n",
    "\\end{align}\n",
    "\n",
    "\n",
    "#### Exc  2.18 'Gaussian Bayes':\n",
    "Derive the above expressions for $P$ and $\\mu$.\n",
    "*Hint: you need eqns (G1) and (3).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('BR Gauss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.20:** Do some light algebra to show that eqns (5) and (6) can be rewritten as\n",
    "\\begin{align}\n",
    "    P &= (1-K)B \\, ,  \\qquad \\qquad (8) \\\\\\\n",
    "  \\mu &= b + K (y-b)  \\qquad \\quad (9) \\, ,\n",
    "\\end{align}\n",
    "where $K = B/(B+R)$, which is called the \"Kalman gain\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.22*:** Consider the formula for $K$ and its role in the previous couple of equations... Why do you think $K$ is called a \"gain\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('KG intuition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.24:** Implement a Gaussian-Gaussian Bayes' rule (eqns 5 and 6, or eqns 8 and 9) by completing the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Bayes_rule_G1(b,B,y,R):\n",
    "    ### INSERT ANSWER HERE ###\n",
    "    return mu,P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('BR Gauss code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.26:** Go back to the above animation code, and uncomment the block that makes use of `Bayes_rule_G1()`. Re-run.  \n",
    "Make sure its curve coincides with that which uses pointwise multiplication (i.e. `Bayes_rule()`).\n",
    "This is the secret behind the Kalman filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.28:** Questions related to the above animation:\n",
    " * Does the width (i.e. scale) for the posterior depend on the location $y$ of the likelihood?\n",
    " * Note that the width (i.e. scale) for the posterior always smaller than that of prior and likelihood.\n",
    "   * What does this mean [information-wise](https://en.wikipedia.org/wiki/Differential_entropy#Differential_entropies_for_various_distributions)?\n",
    "   * Is this ordering also always true for non-Gaussian distributions?\n",
    " * Imagine that you're pretty sure about something, but then you get a wildly different indication (observation).  \n",
    "   What is your posterior uncertainty? Has it decreased?  \n",
    "   So, are you a Gaussian thinker?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('Posterior cov')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.30*:** Why are we so fond of the Gaussian assumption?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('Why Gaussian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next: [Univariate (scalar) Kalman filtering](T3 - Univariate Kalman filtering.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
